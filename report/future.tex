\section{Future Work}
\subsection{Bigger models}
As stated in the report. Our currect distribution of threads makes it impossible for us to process models that use more than 16 * 65535 faces. To solve this problem, we can do a same kind of batching as we did for the memory problem. This means that we just split the input, and make sure that in the different batches the thread id's map to different parts of the input. However, there is a far more efficient way of doing this, by just letting each of the 16 * 65535 threads take care of more than 1 face-face pair. This way, each thread just keeps on running, and subsequent batches do not have to wait for a predecessor to finish. However, we do think that we need bigger hardware, since all the other data will also increase when we use bigger data sets, and then we can get memory problems again, even with all optimizations we did there. \\

On the other hand, would everything fit in memory, then the best solution is to just map more face-face pairs to the same thread id, and let the threads do more work. At this point, we could do other optimizations, addressing coalescing, moving data to shared memory and optimize occupancy, and so on.